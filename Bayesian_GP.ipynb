{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.jupyter-widgets.widget-label {display: none;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.jupyter-widgets.widget-label {display: none;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"w\"\n",
    "matplotlib.rcParams[\"figure.autolayout\"] = True\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "from scipy.stats import binom, beta, multivariate_normal, uniform\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inferance \n",
    "## Combining MCMC and surrogate models\n",
    "\n",
    "### Stéphane Nilsson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Let's first give the mathematical description of Baye's theorem, and then we will go through the parameters and have a concrete exemple.\n",
    "\n",
    "* $P(\\theta | y)= \\frac{P(y|\\theta)P(\\theta)}{P(y)}$\n",
    "  * $y$: Observed data\n",
    "  * $\\theta$: Parameters\n",
    "\n",
    "\n",
    "* $P(\\theta)  :$ Prior probability density\n",
    "* $P(y|\\theta):$ Likelihood\n",
    "* $P(y)       :$ Model evidence\n",
    "* $P(\\theta|y):$ Posterior probability density -> What we are interested in\n",
    "\n",
    "The prior probability density, which I will reduce to prior probability, is the probability of our parameters before taking data into account and quantifies the uncertainty about the parameters. It could be a Uniform distribution which bounds the parameter space to values we know it cannot exceeed. For exemple, if we had to make a linear fit with positive slope, we could bound the values to be positive by setting a Uniform distribution in [0, 1e5].\n",
    "\n",
    "The likelihood tells you how likely the data you have could have been generated by the parameter. Continuing on the linear regression, let's suppose the real data is coming from y=3x. Then the likelihood function would be greated if my parameter of the problem, the slope, is 2.5 rather than 0.5, since on average each point would be closer to my y=2.5x than to my y=0.5x line. The likelihood function is problem dependant as we will see in the following exemples.\n",
    "\n",
    "The model evidence is the probability of having the data given the model. Known also as the marginal likelihood, it is a constant given a single model. This will give our normalizing constant to have a proper probability distribution.\n",
    "\n",
    "The posterior probability is the probability that our parameters have effectively generated the data. In the case of bayseian inferance, we have the data and would like to obtain the underlying parameters, therefore this is the value we eventually want to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a toy example : $$y=4x+1+\\epsilon$$\n",
    "\n",
    "where we set $\\epsilon \\sim \\mathcal{N}(0, 0.5)$\n",
    "\n",
    "In that case we can define our parameter vector $\\theta = \\begin{bmatrix} a \\\\ b \\end{bmatrix} $ such that $\\mathbf{y}^*=\\theta^T\\cdot\\mathbf{x}$ are our estimated values.\n",
    "\n",
    "We would therefore estimate the best values of a and b that fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fede2b46f524e3bad527979bd31ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x=np.linspace(0,1)\n",
    "y=4*x+1\n",
    "x_train=np.random.random(10)\n",
    "y_train=4*x_train+1+np.random.normal(0,0.5,10)\n",
    "fig, ax=plt.subplots(1,1)\n",
    "\n",
    "\n",
    "ax.plot(x_train, y_train, '.', label='Training data points', color='purple')\n",
    "ax.plot(x,y, label='True function', color='orange')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our problem specifications :\n",
    "\n",
    "* We want to evaluate the parameter $\\theta$ of our linear regression. We remind that it is composed of the slope and the intercept, $a$ and $b$.\n",
    "* Looking at the data point, we already know that we have a positive slope. Other than that, we can only estimate a range for the values. We propose a Uniform distribution as a prior for both and specify $P(a)$=Uniform(0,10) and $P(b)=$Uniform(-5,5).\n",
    "* The likelihood is given by $P(y|\\theta)=\\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}\\exp\\left(-\\frac{1}{2\\sigma^2}(\\mathbf{y}- \\mathbf{y}^*)^{\\rm T}(\\mathbf{y}- \\mathbf{y}^*)\\right)$ with $n$ the number of data points. (10 in this example)\n",
    "* Finally, we get the posterior $P(\\theta|\\mathbf{y})\\sim P(y|\\theta)P(a)P(b)$\n",
    "\n",
    "For numerical stability, we can instead compute the log-likelihood $$\\log(P(\\theta|\\mathbf{y})= \\log(P(y|\\theta))+\\log(P(a))+\\log(P(b))$$\n",
    "With $\\log(P(a))$ and $\\log(P(b))$ being constant, we can remove them from the equation. We thus get $$\\log(P(\\theta|\\mathbf{y}))\\sim -\\frac{1}{2\\sigma^2}(\\mathbf{y}-\\mathbf{y^*})^2$$\n",
    "\n",
    "The best parameters would therefore minimize the log-likelihood. One the plot below, a colormap of the likelihood as a function of the two parameters can be created. The best parameters will be in dark purple. A contour map can also be plotted to give an idea of how the likelihood function is varying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08ab514d6d2441293056010a1484921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9555999cb44c09967a6e20900dc5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Slope a: ', max=10, min=-10), IntSlider(value=1, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "x=np.linspace(0,1,2)\n",
    "y=4*x+1\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"plasma\")\n",
    "\n",
    "cmap.set_bad(color='white')\n",
    "\n",
    "\n",
    "flag=np.ones((21,21))\n",
    "flag_colormap=1\n",
    "output = widgets.Output()\n",
    "\n",
    "def posterior_(a, b, y):\n",
    "    y = prior_(a,b) * likelihood_(y)\n",
    "    return y \n",
    "\n",
    "def log_posterior_(y):\n",
    "    y = log_likelihood(y)\n",
    "    return y \n",
    "\n",
    "def likelihood_(y):\n",
    "    y = multivariate_normal.pdf(y,mean=y_train,cov=0.5)\n",
    "    return y\n",
    "\n",
    "def log_likelihood(y):\n",
    "    y = -multivariate_normal.logpdf(y,mean=y_train,cov=0.5)\n",
    "    return y\n",
    "\n",
    "# Just showing how we can compute the uniform probability with scipy\n",
    "def prior_(a ,b ):\n",
    "    return uniform.pdf(a, 0, 10)*uniform(b, -5 ,5)\n",
    "###############################################################################\n",
    "z_post=np.array([log_posterior_(a*x_train+b) for a in np.linspace(-10,10,21) for b in np.linspace(-10,10,21)])\n",
    "z_post=z_post.reshape((21,21))\n",
    "#Make the transition smoother between colors\n",
    "z_post=np.log(z_post)\n",
    "###############################################################################\n",
    "@interact(\n",
    "    a=widgets.IntSlider(min=-10, max=10, value=1, description=\"Slope a: \"),\n",
    "    b=widgets.IntSlider(min=-10, max=10, value=1, description=\"Intercept b: \"),\n",
    ")\n",
    "def update(a, b):\n",
    "    # [l.remove() for l in ax.lines]\n",
    "    # [l.remove() for l in ax.lines]\n",
    "    # Clear axis\n",
    "    global flag_colormap\n",
    "    axs[0].cla()\n",
    "    # Check if ax.collections is empty\n",
    "    # if len(ax.collections):\n",
    "    #     ax.collections.pop()\n",
    "    y_post= a*x+b\n",
    "    flag[a+10,b+10]=0\n",
    "    z_masked=np.ma.masked_where(flag, z_post)\n",
    "    #z_post[a+9,b+9]=log_posterior(y_post_train)\n",
    "\n",
    "\n",
    "    axs[0].plot(x_train, y_train, '.', label='Observered data', color='purple')\n",
    "    axs[0].plot(x, y_post, label='Regression line', color='orange')\n",
    "\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[0].set_ylim(\n",
    "        (np.min(np.r_[y, y_post]) - 1e-1, np.max(np.r_[y, y_post]) + 1e-1)\n",
    "    )\n",
    "    axs[0].set_xlim((0, 1))\n",
    "\n",
    "    img=axs[1].imshow(z_masked,cmap=cmap, extent=[-10,10,10,-10],vmin=np.min(z_post), vmax=np.max(z_post))\n",
    "    axs[1].set_xticks(ticks=np.linspace(-10,10,11))\n",
    "    axs[1].set_yticks(ticks=np.linspace(-10,10,11))\n",
    "    axs[1].set_xlabel('b', fontsize=16)\n",
    "    axs[1].set_ylabel('a', fontsize=16)\n",
    "    #Avoid plotting the colormap multiple times\n",
    "    if flag_colormap:\n",
    "        plt.colorbar(img,ax=axs[1],shrink=0.75);\n",
    "        flag_colormap=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a11b84279440b69d40286b9c705568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter set is a=4.00 and b=1.20\n"
     ]
    }
   ],
   "source": [
    "fig, ax=plt.subplots(1,1)\n",
    "param=np.linspace(-10,10,101)\n",
    "z_post2=np.array([log_posterior_(a*x_train+b) for a in param for b in param])\n",
    "z_post2=z_post2.reshape((101,101))\n",
    "#Make the transition smoother between colors\n",
    "z_post2=np.log(z_post2)\n",
    "row, col = np.unravel_index(z_post2.argmin(), z_post2.shape)\n",
    "print(f'The best parameter set is a={param[row]:0.2f} and b={param[col]:0.2f}')\n",
    "ax.contour(param,param,z_post2,cmap=cmap)\n",
    "ax.plot(param[col],param[row],'k*', ms=8 ,label='Best parameters')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin flip exemple\n",
    "Context : You are at a fair and on one of the stand, a mysterious person tells you to come by. He tells you the following simple rules : he will throw the coin 10 times in the air. If you guess correctly the number of head, you double your bet, if the number of head is higher, then you lose your bet, if the number is lower, you get back your bet. However you feel like something is wrong and that the coin is biased. But to make sure, you have to gather evidence, and therefore wait to see 5 people playing before you. We will now apply Bayesian reasoning to see if your intuition was real.\n",
    "\n",
    "In the context of a coin flip, we can compute what is the probability of obtaining N heads out of 10 throws using the Binomial distribution Binomial(N,10,p), where $p=0.5$ is the probability of getting a head. In a fair coin case, the probability $p$ is 0.5. Below is a graph showing the probability of getting N heads out of 10 throws for N=0...10. In that case the probability is highest to get 5 heads and 5 tails, which makes sense since p=0.5. Getting 8 heads has a 5% chance, but it's not impossible. Therefore, if participants are losing, is it bad luck or is there something else going on ? Let's explore this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fc3d40006f4557aca5ee9cd8d0279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax= plt.subplots(1,1)\n",
    "x=np.arange(0,11)\n",
    "ax.plot(x, binom.pmf(x, 10, 0.5), 'bo', ms=5)\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Number of heads');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to define our problem. Keep in mind that we want to evaluate the parameter $p$ of the Binomial distribution, which is a probability distribution. In the end, we will have a probability distribution of the value $p$, that is we will not get the best value of $p$, but a plausible range.\n",
    "\n",
    "* We want to evaluate the probability $p$ of getting a head ($\\mathrm{H}$)\n",
    "* We start by assuming we have no prior information on the coin, thus we set the prior $P(p)$=Beta(1,1) (Equivalent to Uniform distribution)\n",
    "* The likelihood is given by $P(\\mathrm{H}|p)$=Binomial($\\mathrm{H}$, $\\mathrm{H+T}$, $p$)\n",
    "* Finally, we get the posterior $P(p|H)\\sim $ Beta(1,1)xBinomial($\\mathrm{H}$, $\\mathrm{H+T}$, $p$)\n",
    "\n",
    "For an analytical derivation of the solution, visit [Bayesian Coin Flip](https://nbviewer.org/github/lambdafu/notebook/blob/master/math/Bayesian%20Coin%20Flip.ipynb)\n",
    "\n",
    "For more in-depth explanations with Python code, I recommand [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cdc37746c54c35bbfab96a16097796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99d1b040cc24ec499d84b4410cfc24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Number of heads: ', max=50, style=SliderStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "\n",
    "def posterior(H, T, a, b, x):\n",
    "    y = prior(a, b, x) * likelihood(H, T, x)\n",
    "    return y / integrate.simpson(y, x)\n",
    "\n",
    "\n",
    "def likelihood(H, T, x):\n",
    "    # Normalize likelihood to make it look nicer\n",
    "    y = binom.pmf(H, H + T, [x]).reshape(-1,)\n",
    "    return y / integrate.simpson(y, x)\n",
    "\n",
    "\n",
    "def prior(a, b, x):\n",
    "    return beta.pdf(x, a, b)\n",
    "\n",
    "\n",
    "@interact(\n",
    "    H=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=50,\n",
    "        value=0,\n",
    "        description=\"Number of heads: \",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    ),\n",
    "    T=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=50,\n",
    "        value=0,\n",
    "        description=\"Number of tails:   \",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    ),\n",
    "    a=widgets.IntSlider(min=1, max=10, value=1, description=\"a: \"),\n",
    "    b=widgets.IntSlider(min=1, max=10, value=1, description=\"b: \"),\n",
    ")\n",
    "def update(H, T, a, b):\n",
    "    # [l.remove() for l in ax.lines]\n",
    "    # [l.remove() for l in ax.lines]\n",
    "    # Clear axis\n",
    "    ax.cla()\n",
    "    # Check if ax.collections is empty\n",
    "    # if len(ax.collections):\n",
    "    #     ax.collections.pop()\n",
    "    x = np.linspace(0, 1, 200)\n",
    "\n",
    "    y_prior = prior(a, b, x)\n",
    "    y_likelihood = likelihood(H, T, x)\n",
    "    y_post = posterior(H, T, a, b, x)\n",
    "\n",
    "    ax.plot(x, y_prior, \"--\", c=\"blue\", label=\"Prior\", alpha=0.6)\n",
    "    ax.plot(x, y_likelihood, \"--\", c=\"red\", label=\"Likelihood\", alpha=0.6)\n",
    "    ax.plot(x, y_post, c=\"violet\", label=\"Posterior\")\n",
    "    ax.fill_between(x, x * 0, y_post, color=\"violet\", alpha=0.2)\n",
    "\n",
    "    ax.set_xlabel(\"$p$\")\n",
    "    ax.set_ylabel(\"PDF\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_ylim(\n",
    "        (np.min(y_post) - 1e-1, np.max(np.r_[y_post, y_likelihood, y_prior]) + 1e-1)\n",
    "    )\n",
    "    ax.set_xlim((0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the relation $P(\\theta|y)\\propto P(\\theta)*P(y|\\theta)$, the only thing missing is $P(y)$ which will act as the normalizing constant. However, computing $P(y)=\\int P(y|\\theta)d\\theta$ requires computing an integral. Numerous methods are readily available when computing one-dimensional integrals, however, in an N-dimensional space (that is we have N parameters), the integral is computationaly too expensive, due to the curse of dimensionality. Therefore other methods can be used, such as the MCMC class of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we cannot compute directly $P(\\theta|y)$?\n",
    "### Markov chain Monte Carlo (MCMC)\n",
    "\n",
    "Monte Carlo : Generation of random numbers from a given distribution\n",
    "\n",
    "Markov Chain : Sequence of number where each number is dependant on the previous number\n",
    "\n",
    "Metropolis-Hastings : Defining an acceptance rate of the new number\n",
    "\n",
    "The algorithm will sample the N-dimensional space with a probability proportional to our posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algorithm :\n",
    "1. Initialize with an arbitrary set of parameters $\\theta$ and set a probability distribution function from which the next candidate $\\theta'$ will be picked from. Usually it is a Gaussian centered around $\\theta$.\n",
    "2. Randomly pick a new candidate $\\theta'$ following the distribution.\n",
    "3. Compute the acceptance probability $\\alpha=\\frac{P(y|\\theta')P(\\theta')}{P(y|\\theta)P(\\theta)}$\n",
    "4. Accept or reject :\n",
    "  * Draw a random number $u \\in [0,1]$\n",
    "  * If $\\alpha>u$ , accept and set $\\theta_{t+1}=\\theta'$\n",
    "  * If $\\alpha<u$ , reject and set $\\theta_{t+1}=\\theta$\n",
    "\n",
    "Discard the first $N$ values as burn-in period to keep converged values.\n",
    "\n",
    "For more information about different sampling techniques, visit [MCMC Algorithms](https://m-clark.github.io/docs/ld_mcmc/index_onepage.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the algorithm to the previous example and compare to analytical solution as the number of samples grows.\n",
    "\n",
    "This time we assume that the coin is biased towards Heads, and thus set the prior distribution to Beta(10,5).\n",
    "\n",
    "Five participants flip the coin and obtain 28 $\\mathrm{H}$ out of the total 50 throws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMC(n_samples, accepted, rejected):\n",
    "    a, b = 10, 5\n",
    "    H, T = 28, 22\n",
    "\n",
    "    #If chain already started, continue the chain\n",
    "    if len(accepted):\n",
    "        p_old=accepted[-1]\n",
    "\n",
    "    # Otherwise, sample intial number from prior\n",
    "    else:\n",
    "        p_old = beta.rvs(a=10, b=5, size=1)\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Sample new number from Gaussian distribution centered around p_old\n",
    "        p_new = multivariate_normal.rvs(mean=p_old, cov=0.01)\n",
    "\n",
    "        # Our likelihood and prior functions\n",
    "        likelihood = lambda p: binom.pmf(H, H + T, p).reshape(-1,)\n",
    "        prior = lambda p: beta.pdf(p, a, b)\n",
    "        # Acceptance\n",
    "        alpha = likelihood(p_new) * prior(p_new) / (likelihood(p_old) * prior(p_old))\n",
    "\n",
    "        u = np.random.random(1)\n",
    "        if alpha > u :\n",
    "            accepted.append(p_new)\n",
    "            p_old = p_new\n",
    "        else:\n",
    "            rejected.append(p_new)\n",
    "\n",
    "    return accepted, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a38366a49374f70b00271a6ae1cd0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e461f1e11c45f9a24fb18ab279cada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Number of samples: ', options=((1000, 1000), (2000, 2000), (5000, 5000)), style=Descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9be670465724ba68ff02fd6be43adea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Draw samples', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679aa91b25b148dbb3e165c60b1b6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import functools\n",
    "\n",
    "fig2, ax2=plt.subplots(1,1)\n",
    "accepted=[]\n",
    "rejected=[]\n",
    "\n",
    "x=np.linspace(0,1,100)\n",
    "y_analytic = posterior(28,22,10,5,x)\n",
    "burn_in=100\n",
    "dropdown=widgets.Dropdown(\n",
    "    options=[(1000, 1000), (2000, 2000), (5000, 5000)],\n",
    "    value=1000,\n",
    "    description='Number of samples: ',\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Draw samples\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(dropdown,button, output)\n",
    "\n",
    "def update(accepted, rejected):\n",
    "    with output:\n",
    "        n_samples=dropdown.value\n",
    "        accepted, rejected = MCMC(n_samples, accepted, rejected)\n",
    "        ax2.cla()\n",
    "        ax2.hist(accepted[burn_in:], bins=50,density=True, histtype='step', color='blue', label='Drawn samples');\n",
    "        ax2.plot(x,y_analytic, label='Analytical posterior', color='violet')\n",
    "        ax2.fill_between(x, x*0, y_analytic, color=\"violet\",alpha=0.2)\n",
    "        ax2.legend()\n",
    "        ax2.set_xlabel('$p$')\n",
    "        ax2.set_ylabel('PDF')\n",
    "        \n",
    "def on_button_clicked(b, accepted, rejected):\n",
    "    update(accepted, rejected)\n",
    "\n",
    "\n",
    "button.on_click(functools.partial(on_button_clicked, accepted=accepted, rejected=rejected))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The case of the laser model\n",
    "\n",
    "In the case of a time series, where we suppose $$y=\\theta\\cdot\\mathbf{x} + \\epsilon$$ with $$\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$$\n",
    "the likelihood function is given by $$\\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf y^*}-{\\mathbf y})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf y^*}-{\\mathbf y})\\right)}{\\sqrt{(2\\pi)^k |\\boldsymbol\\Sigma|}} $$ with $y^*$ the predicted data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously, the MCMC algorithm evaluates the likelihood function for every new sample we draw. That means, we would have to compute a new FEM simulation for every new set of parameters, which is not feasable due to time constrains.\n",
    "\n",
    "A way to avoid computing the FEM simulation everytime is to use surrogate models. A class of surrogate model is Gaussian Process Regression, which will be presented in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "The aim of the Gaussian Regression is to model the function as a multivariate Gaussian. \n",
    "A Gaussian process is a stochastic process with Gaussian distributions $$(y(x_1), y(x_2), ..., y(x_n)) \\sim \\mathcal{N}_n(\\boldsymbol{\\mu},\\Sigma)$$\n",
    "\n",
    "To specify a Gaussian distribution, we only need mean and variance $$X \\sim \\mathcal{N}(\\mu, \\Sigma)$$\n",
    "\n",
    "To specify a Gaussian _process_, we need a mean and covariance _function_ $$y(\\cdot) \\sim GP(m(\\cdot), k(\\cdot,\\cdot))$$\n",
    "\n",
    "We are free to choose mean and covariance function, however they should be subject to some rules\n",
    "\n",
    "Popular choices for mean are :\n",
    "* m(x)=0\n",
    "* m(x)=const\n",
    "* m(x)=$\\beta^T\\mathbf{x}$\n",
    "\n",
    "The kernel, which is a function of the location $$k(x,x')=\\mathbb{C}ov(y(x),y(x'))$$\n",
    "must be positive semi-definite to lead to a valid covariance matrix.\n",
    "\n",
    "The choice of the kernel will dictate the space of functions that can represent our GP. A widely used kernel is the RBF kernel given by $$k(x,x')=\\sigma^2\\exp(-\\frac{\\|x-x'\\|^2}{2l^2})$$\n",
    "\n",
    "In the kernel you will notice that we have two parameters $\\sigma$ and $l$. They are the variance and lengthscale of the kernel respectively. Those are two hyperparameters which we will need to tune in order to fit best the data. \n",
    "\n",
    "In a Gaussian regression, we start by first assuming the multivariate Gaussian has mean vector of zero and variance one, that is $$(y(x_1), y(x_2), ..., y(x_n)) \\sim \\mathcal{N}_n(\\boldsymbol{0},I)$$\n",
    "\n",
    "The multivariante gaussian distribution is then updated _conditional_ on the observed data point. That is, we update the mean vector and covariance matrix of the multivariate Gaussian with the newly acquired information.\n",
    "\n",
    "For more in-depth details in how the conditional update is applied and the origin of the kernel, I strongly recommend going to [Gaussian Process Summer School](http://gpss.cc/gpss21/program)\n",
    "\n",
    "While this all seems pretty abstract for now, let us explore the effect of using different kernel and their respective hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://www.infinitecuriosity.org/vizgp/\" width=\"1200\" height=\"1000\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"http://www.infinitecuriosity.org/vizgp/\" width=\"1200\" height=\"1000\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the fun is over, I would like to add more to what can be done with kernels.\n",
    "\n",
    "A property of those kernels is that they can be added or multiplied together and still lead to positive semi-definite covoriance matrices. A discussion about kernel choice is given in [Kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/)\n",
    "\n",
    "Kernel are meant to pass _exactly_ on observed data point. However, if we know the variance of the observed data point, we can incorporate it in the model by adding a WhiteKernel to the kernel choice, which will simply add a variance term in the diagonal of the covariance matrix.\n",
    "\n",
    "Until now, we have had function from $\\mathbb{R}\\rightarrow\\mathbb{R}$. But we can also apply the same procedure for function $\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$ or $\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$. In that case, we can choose to either have the same kernel for all dimensions, or select different kernel for different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPy for Gaussian Process Regression\n",
    "\n",
    "Let us now see how all of this works in Python\n",
    "\n",
    "We will start by import the relevant libraries and creating a dummy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPy: Gaussian processes library\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our Gaussian process prior with some observed data to form a GP regression model. \n",
    "\n",
    "Suppose we have a data model for which we only have noisy observations, $y = f(x) + \\epsilon$ at some small number of sample locations, $\\mathbf{X}$. Here, we set up an example function\n",
    "\n",
    "$$\n",
    "    f(x) = -\\cos(2\\pi x) + \\frac{1}{2}\\sin(6\\pi x)\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{y} = f(\\mathbf{X}) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.01)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98153b1408b4a9c80452b7cb098337b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lambda function, call f(x) to generate data\n",
    "f = lambda x: -np.cos(2 * np.pi * x) + 0.5 * np.sin(6 * np.pi * x)\n",
    "\n",
    "# 10 equally spaced sample locations\n",
    "X = np.linspace(0.05, 0.95, 10)[:, None]\n",
    "\n",
    "# y = f(X) + epsilon\n",
    "Y = f(X) + np.random.normal(\n",
    "    0.0, 0.1, (10, 1)\n",
    ")  # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01\n",
    "\n",
    "# Plot observations\n",
    "fig3, ax3=plt.subplots(1,1)\n",
    "ax3.plot(X, Y, \"kx\", mew=2)\n",
    "\n",
    "# Annotate plot\n",
    "ax3.set_xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "ax3.legend(labels=[\"sample points\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit a GP regression to the data using GPy. For that we will use a RBF kernel and optimize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/3, f = 9.55749646765872\n",
      "Optimization restart 2/3, f = 9.557495378215801\n",
      "Optimization restart 3/3, f = 9.557495528670088\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".pd{\n",
       "    font-family: \"Courier New\", Courier, monospace !important;\n",
       "    width: 100%;\n",
       "    padding: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<p class=pd>\n",
       "<b>Model</b>: GP regression<br>\n",
       "<b>Objective</b>: 9.557495378215801<br>\n",
       "<b>Number of Parameters</b>: 3<br>\n",
       "<b>Number of Optimization Parameters</b>: 3<br>\n",
       "<b>Updates</b>: True<br>\n",
       "</p>\n",
       "<style type=\"text/css\">\n",
       ".tg  {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\"><tr><th><b>  GP_regression.         </b></th><th><b>                value</b></th><th><b>constraints</b></th><th><b>priors</b></th></tr>\n",
       "<tr><td class=tg-left>  rbf.variance           </td><td class=tg-right>   0.7124504743054618</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  rbf.lengthscale        </td><td class=tg-right>  0.10034634983104936</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  Gaussian_noise.variance</td><td class=tg-right>4.717638732837407e-09</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<GPy.models.gp_regression.GPRegression at 0x28a6a263550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the kernel\n",
    "k = GPy.kern.RBF(1, variance=1.0, lengthscale=0.1, name=\"rbf\")\n",
    "# Create the GP regression model\n",
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "# Optimize the kernel hyperparameters, the lengthscale and the variance\n",
    "m.optimize_restarts(3)\n",
    "# Look at the model parameters\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(X, m, C, training_points=None):\n",
    "    \"\"\" Plotting utility to plot a GP fit with 95% confidence interval \"\"\"\n",
    "    # Plot 95% confidence interval\n",
    "    plt.fill_between(\n",
    "        X[:, 0],\n",
    "        m[:, 0] - 1.96 * np.sqrt(np.diag(C)),\n",
    "        m[:, 0] + 1.96 * np.sqrt(np.diag(C)),\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    # Plot GP mean and initial training points\n",
    "    plt.plot(X, m, \"-\")\n",
    "    plt.legend(labels=[\"GP fit\"])\n",
    "\n",
    "    plt.xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "\n",
    "    # Plot training points if included\n",
    "    if training_points is not None:\n",
    "        X_, Y_ = training_points\n",
    "        plt.plot(X_, Y_, \"kx\", mew=2)\n",
    "        plt.legend(labels=[\"GP fit\", \"sample points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeeaf01371d474d90452ecae19ea848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xnew = np.linspace(-0.05, 1.05, 100)[:, None]\n",
    "# Predict new data points\n",
    "# Get mean and covariance of optimised GP\n",
    "mean, Cov = m.predict_noiseless(Xnew, full_cov=True)\n",
    "\n",
    "# Plot the GP fit mean and covariance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X, Y))\n",
    "plt.plot(Xnew, f(Xnew), \"r:\", lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on how to use GPy, please visit [GPSS Lab](http://gpss.cc/gpss21/labs) for a general introduction, [GP Examples](https://nbviewer.org/github/SheffieldML/notebook/blob/master/GPy/index.ipynb) for specific examples on how to use GPy and the documentation can be found here : [GPy Docs](https://gpy.readthedocs.io/en/deploy/).\n",
    "\n",
    "While GPy is stable, it is no longer under active developement. Alternatives are [GPytorch](https://gpytorch.ai/) and [GPyFlow](https://gpflow.readthedocs.io/en/master/intro.html) based on Pytorch and TensorFlow respectively, to leverage GPU speed up and training on big data sets. It should be noted that a less extensive GP Regression is provided by [Scikit-learn](https://scikit-learn.org/stable/modules/gaussian_process.html). Another library is [PyMC3](https://docs.pymc.io/en/stable/) which is specialized in probabilistic learning. I find the latter to be the least intuitive of them all to set up a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "hide_input": false,
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
